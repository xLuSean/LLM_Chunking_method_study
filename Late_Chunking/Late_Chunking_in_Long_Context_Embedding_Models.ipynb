{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1173893c4f0ea56",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "e1173893c4f0ea56"
      },
      "source": [
        "# [Late Chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models)\n",
        "\n",
        "This notebooks explains how the \"Late Chunking\" can be implemented. First you need to install the requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02a920f-cde0-4035-9834-49b087aab5cc",
      "metadata": {
        "is_executing": true,
        "id": "d02a920f-cde0-4035-9834-49b087aab5cc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.43.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a8fbc1e477db48",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "58a8fbc1e477db48"
      },
      "source": [
        "Then we load a model which we want to use for the embedding. We choose `jinaai/jina-embeddings-v2-base-en` but any other model which supports mean pooling is possible. However, models with a large maximum context-length are preferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1380abf7acde9517",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1380abf7acde9517",
        "outputId": "ef33f63d-535b-44ec-c1b0-5c06815c716a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/michael/workspace/chunked-pooling/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc0c1162797ffb0",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2cc0c1162797ffb0"
      },
      "source": [
        "Now we define the text which we want to encode and split it into chunks. The `chunk_by_sentences` function also returns the span annotations.\n",
        "Those specify the number of tokens per chunk which is needed for the chunked pooling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
        "    \"\"\"\n",
        "    Split the input text into sentences using the tokenizer\n",
        "    :param input_text: The text snippet to split into sentences\n",
        "    :param tokenizer: The tokenizer to use\n",
        "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
        "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
        "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
        "    token_offsets = inputs['offset_mapping'][0]\n",
        "    token_ids = inputs['input_ids'][0]\n",
        "    chunk_positions = [\n",
        "        (i, int(start + 1))\n",
        "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
        "        if token_id == punctuation_mark_id\n",
        "        and (\n",
        "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
        "            or token_ids[i + 1] == sep_id\n",
        "        )\n",
        "    ]\n",
        "    chunks = [\n",
        "        input_text[x[1] : y[1]]\n",
        "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
        "    ]\n",
        "    span_annotations = [\n",
        "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
        "    ]\n",
        "    return chunks, span_annotations"
      ],
      "metadata": {
        "id": "MNi-3U1amWTa"
      },
      "id": "MNi-3U1amWTa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In production, you should use more advanced and robust segmentation method such as Jina AI Tokenizer API https://jina.ai/tokenizer#apiform."
      ],
      "metadata": {
        "id": "-UNs-ggom9Zq"
      },
      "id": "-UNs-ggom9Zq"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def chunk_by_tokenizer_api(input_text: str, tokenizer: callable):\n",
        "    # Define the API endpoint and payload\n",
        "    url = 'https://tokenize.jina.ai/'\n",
        "    payload = {\n",
        "        \"content\": input_text,\n",
        "        \"return_chunks\": \"true\",\n",
        "        \"max_chunk_length\": \"1000\"\n",
        "    }\n",
        "\n",
        "    # Make the API request\n",
        "    response = requests.post(url, json=payload)\n",
        "    response_data = response.json()\n",
        "\n",
        "    # Extract chunks and positions from the response\n",
        "    chunks = response_data.get(\"chunks\", [])\n",
        "    chunk_positions = response_data.get(\"chunk_positions\", [])\n",
        "\n",
        "    # Adjust chunk positions to match the input format\n",
        "    span_annotations = [(start, end) for start, end in chunk_positions]\n",
        "\n",
        "    return chunks, span_annotations"
      ],
      "metadata": {
        "id": "NWfPSUUVmYB4"
      },
      "id": "NWfPSUUVmYB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to segement a toy example."
      ],
      "metadata": {
        "id": "2JyrW8uunIrj"
      },
      "id": "2JyrW8uunIrj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef392f3437ef82e",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8ef392f3437ef82e",
        "outputId": "8b350526-0b66-442f-9552-f227af03679d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks:\n",
            "- \"Berlin is the capital and largest city of Germany, both by area and by population.\"\n",
            "- \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"\n",
            "- \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
        "\n",
        "# determine chunks\n",
        "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
        "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac41fd1f0560da7",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "9ac41fd1f0560da7"
      },
      "source": [
        "Now we encode the chunks with the traditional and the context-sensitive late_chunking method:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def late_chunking(\n",
        "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
        "):\n",
        "    token_embeddings = model_output[0]\n",
        "    outputs = []\n",
        "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
        "        if (\n",
        "            max_length is not None\n",
        "        ):  # remove annotations which go bejond the max-length of the model\n",
        "            annotations = [\n",
        "                (start, min(end, max_length - 1))\n",
        "                for (start, end) in annotations\n",
        "                if start < (max_length - 1)\n",
        "            ]\n",
        "        pooled_embeddings = [\n",
        "            embeddings[start:end].sum(dim=0) / (end - start)\n",
        "            for start, end in annotations\n",
        "            if (end - start) >= 1\n",
        "        ]\n",
        "        pooled_embeddings = [\n",
        "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
        "        ]\n",
        "        outputs.append(pooled_embeddings)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "GOPvzV4rlozA"
      },
      "id": "GOPvzV4rlozA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe3d93b9e6609b9",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "abe3d93b9e6609b9"
      },
      "outputs": [],
      "source": [
        "# chunk before\n",
        "embeddings_traditional_chunking = model.encode(chunks)\n",
        "\n",
        "# chunk afterwards (context-sensitive chunked pooling)\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "model_output = model(**inputs)\n",
        "embeddings = late_chunking(model_output, [span_annotations])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e84b1b9d48cb6367",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "e84b1b9d48cb6367"
      },
      "source": [
        "Finally, we compare the similarity of the word \"Berlin\" with the chunks. The similarity should be higher for the context-sensitive chunked pooling method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0cec59a3ece76",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "da0cec59a3ece76",
        "outputId": "b3d0e805-0c59-4794-dab3-ae2a6b4aa77a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similarity_new(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.849546\n",
            "similarity_trad(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.84862185\n",
            "similarity_new(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.82489026\n",
            "similarity_trad(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.7084338\n",
            "similarity_new(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.84980094\n",
            "similarity_trad(\"Berlin\", \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"): 0.7534553\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "\n",
        "berlin_embedding = model.encode('Berlin')\n",
        "\n",
        "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
        "    print(f'similarity_new(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, new_embedding))\n",
        "    print(f'similarity_trad(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, trad_embeddings))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}